{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_features_path = 'featuresvalues.csv'\n",
    "tumor_features_path = 'tumorfeatures.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>4.1</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 98</th>\n",
       "      <th>Unnamed: 99</th>\n",
       "      <th>Unnamed: 100</th>\n",
       "      <th>Unnamed: 101</th>\n",
       "      <th>Unnamed: 102</th>\n",
       "      <th>Unnamed: 103</th>\n",
       "      <th>Unnamed: 104</th>\n",
       "      <th>Unnamed: 105</th>\n",
       "      <th>Unnamed: 106</th>\n",
       "      <th>Unnamed: 107</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.312711</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96.625630</td>\n",
       "      <td>85.937500</td>\n",
       "      <td>30.937500</td>\n",
       "      <td>90.336624</td>\n",
       "      <td>90.336624</td>\n",
       "      <td>1429.723612</td>\n",
       "      <td>30.215894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021158</td>\n",
       "      <td>4.950082</td>\n",
       "      <td>0.140360</td>\n",
       "      <td>1755.084130</td>\n",
       "      <td>2.492708</td>\n",
       "      <td>0.004271</td>\n",
       "      <td>23.053272</td>\n",
       "      <td>0.013157</td>\n",
       "      <td>0.220057</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.702091</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79.821001</td>\n",
       "      <td>61.992188</td>\n",
       "      <td>65.585938</td>\n",
       "      <td>78.452682</td>\n",
       "      <td>78.452682</td>\n",
       "      <td>2318.384043</td>\n",
       "      <td>56.041604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014458</td>\n",
       "      <td>5.678697</td>\n",
       "      <td>0.136410</td>\n",
       "      <td>559.907954</td>\n",
       "      <td>2.690750</td>\n",
       "      <td>0.003352</td>\n",
       "      <td>34.023626</td>\n",
       "      <td>0.019771</td>\n",
       "      <td>0.225209</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.691442</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67.548490</td>\n",
       "      <td>64.414062</td>\n",
       "      <td>51.796875</td>\n",
       "      <td>70.155304</td>\n",
       "      <td>70.155304</td>\n",
       "      <td>2198.023097</td>\n",
       "      <td>46.705885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020683</td>\n",
       "      <td>7.113756</td>\n",
       "      <td>0.159100</td>\n",
       "      <td>1641.650580</td>\n",
       "      <td>0.073083</td>\n",
       "      <td>0.004445</td>\n",
       "      <td>1006.778331</td>\n",
       "      <td>0.121947</td>\n",
       "      <td>20.989892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.207779</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>156.088068</td>\n",
       "      <td>78.164062</td>\n",
       "      <td>35.937500</td>\n",
       "      <td>132.375551</td>\n",
       "      <td>132.375551</td>\n",
       "      <td>1908.062490</td>\n",
       "      <td>32.431834</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027838</td>\n",
       "      <td>5.966807</td>\n",
       "      <td>0.337106</td>\n",
       "      <td>22.895755</td>\n",
       "      <td>1.662553</td>\n",
       "      <td>0.006002</td>\n",
       "      <td>83.262073</td>\n",
       "      <td>0.079386</td>\n",
       "      <td>0.502555</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.282777</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.612953</td>\n",
       "      <td>52.338000</td>\n",
       "      <td>41.184000</td>\n",
       "      <td>95.669886</td>\n",
       "      <td>95.669886</td>\n",
       "      <td>1488.830343</td>\n",
       "      <td>28.451027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025614</td>\n",
       "      <td>5.595331</td>\n",
       "      <td>0.160192</td>\n",
       "      <td>811.998073</td>\n",
       "      <td>1.168050</td>\n",
       "      <td>0.005002</td>\n",
       "      <td>42.698793</td>\n",
       "      <td>0.015685</td>\n",
       "      <td>0.535532</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.193854</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>162.396489</td>\n",
       "      <td>70.312500</td>\n",
       "      <td>26.367188</td>\n",
       "      <td>141.827003</td>\n",
       "      <td>141.827003</td>\n",
       "      <td>2212.206523</td>\n",
       "      <td>31.481219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027410</td>\n",
       "      <td>5.661709</td>\n",
       "      <td>0.243142</td>\n",
       "      <td>131.112058</td>\n",
       "      <td>2.948380</td>\n",
       "      <td>0.003997</td>\n",
       "      <td>73.166907</td>\n",
       "      <td>0.056923</td>\n",
       "      <td>0.501311</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.335641</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.225046</td>\n",
       "      <td>54.687500</td>\n",
       "      <td>48.828125</td>\n",
       "      <td>88.307389</td>\n",
       "      <td>88.307389</td>\n",
       "      <td>1618.226369</td>\n",
       "      <td>33.639606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012035</td>\n",
       "      <td>6.291807</td>\n",
       "      <td>0.299544</td>\n",
       "      <td>35.919704</td>\n",
       "      <td>0.329676</td>\n",
       "      <td>0.007996</td>\n",
       "      <td>423.064167</td>\n",
       "      <td>0.035987</td>\n",
       "      <td>4.055273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.532818</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54.400939</td>\n",
       "      <td>34.179688</td>\n",
       "      <td>41.015625</td>\n",
       "      <td>51.563980</td>\n",
       "      <td>51.563980</td>\n",
       "      <td>652.949015</td>\n",
       "      <td>28.985823</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024009</td>\n",
       "      <td>5.611265</td>\n",
       "      <td>0.248266</td>\n",
       "      <td>48.664024</td>\n",
       "      <td>0.268894</td>\n",
       "      <td>0.027179</td>\n",
       "      <td>92.863090</td>\n",
       "      <td>0.062765</td>\n",
       "      <td>3.732317</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.184894</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172.443819</td>\n",
       "      <td>122.812500</td>\n",
       "      <td>29.062500</td>\n",
       "      <td>133.721156</td>\n",
       "      <td>133.721156</td>\n",
       "      <td>1576.831055</td>\n",
       "      <td>31.883750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030018</td>\n",
       "      <td>5.640576</td>\n",
       "      <td>0.406452</td>\n",
       "      <td>9.481230</td>\n",
       "      <td>1.729093</td>\n",
       "      <td>0.004740</td>\n",
       "      <td>185.259795</td>\n",
       "      <td>0.068346</td>\n",
       "      <td>0.938922</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.444129</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>103.257478</td>\n",
       "      <td>91.298750</td>\n",
       "      <td>50.031150</td>\n",
       "      <td>111.974944</td>\n",
       "      <td>108.974944</td>\n",
       "      <td>3150.920105</td>\n",
       "      <td>57.830814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003056</td>\n",
       "      <td>7.292279</td>\n",
       "      <td>0.277393</td>\n",
       "      <td>82.017864</td>\n",
       "      <td>0.071760</td>\n",
       "      <td>0.004713</td>\n",
       "      <td>1174.227189</td>\n",
       "      <td>0.083357</td>\n",
       "      <td>15.020258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           1  2  3           4           5          6           7           8  \\\n",
       "0   0.312711  0  0   96.625630   85.937500  30.937500   90.336624   90.336624   \n",
       "1   0.702091  0  0   79.821001   61.992188  65.585938   78.452682   78.452682   \n",
       "2   0.691442  0  0   67.548490   64.414062  51.796875   70.155304   70.155304   \n",
       "3   0.207779  0  0  156.088068   78.164062  35.937500  132.375551  132.375551   \n",
       "4   0.282777  0  0  100.612953   52.338000  41.184000   95.669886   95.669886   \n",
       "..       ... .. ..         ...         ...        ...         ...         ...   \n",
       "75  0.193854  0  0  162.396489   70.312500  26.367188  141.827003  141.827003   \n",
       "76  0.335641  0  0  100.225046   54.687500  48.828125   88.307389   88.307389   \n",
       "77  0.532818  0  0   54.400939   34.179688  41.015625   51.563980   51.563980   \n",
       "78  0.184894  0  0  172.443819  122.812500  29.062500  133.721156  133.721156   \n",
       "79  0.444129  0  0  103.257478   91.298750  50.031150  111.974944  108.974944   \n",
       "\n",
       "            4.1  Unnamed: 9  ...  Unnamed: 98  Unnamed: 99  Unnamed: 100  \\\n",
       "0   1429.723612   30.215894  ...     0.021158     4.950082      0.140360   \n",
       "1   2318.384043   56.041604  ...     0.014458     5.678697      0.136410   \n",
       "2   2198.023097   46.705885  ...     0.020683     7.113756      0.159100   \n",
       "3   1908.062490   32.431834  ...     0.027838     5.966807      0.337106   \n",
       "4   1488.830343   28.451027  ...     0.025614     5.595331      0.160192   \n",
       "..          ...         ...  ...          ...          ...           ...   \n",
       "75  2212.206523   31.481219  ...     0.027410     5.661709      0.243142   \n",
       "76  1618.226369   33.639606  ...     0.012035     6.291807      0.299544   \n",
       "77   652.949015   28.985823  ...     0.024009     5.611265      0.248266   \n",
       "78  1576.831055   31.883750  ...     0.030018     5.640576      0.406452   \n",
       "79  3150.920105   57.830814  ...     0.003056     7.292279      0.277393   \n",
       "\n",
       "    Unnamed: 101  Unnamed: 102  Unnamed: 103  Unnamed: 104  Unnamed: 105  \\\n",
       "0    1755.084130      2.492708      0.004271     23.053272      0.013157   \n",
       "1     559.907954      2.690750      0.003352     34.023626      0.019771   \n",
       "2    1641.650580      0.073083      0.004445   1006.778331      0.121947   \n",
       "3      22.895755      1.662553      0.006002     83.262073      0.079386   \n",
       "4     811.998073      1.168050      0.005002     42.698793      0.015685   \n",
       "..           ...           ...           ...           ...           ...   \n",
       "75    131.112058      2.948380      0.003997     73.166907      0.056923   \n",
       "76     35.919704      0.329676      0.007996    423.064167      0.035987   \n",
       "77     48.664024      0.268894      0.027179     92.863090      0.062765   \n",
       "78      9.481230      1.729093      0.004740    185.259795      0.068346   \n",
       "79     82.017864      0.071760      0.004713   1174.227189      0.083357   \n",
       "\n",
       "    Unnamed: 106  Unnamed: 107  \n",
       "0       0.220057             0  \n",
       "1       0.225209             0  \n",
       "2      20.989892             0  \n",
       "3       0.502555             0  \n",
       "4       0.535532             0  \n",
       "..           ...           ...  \n",
       "75      0.501311             0  \n",
       "76      4.055273             0  \n",
       "77      3.732317             0  \n",
       "78      0.938922             0  \n",
       "79     15.020258             0  \n",
       "\n",
       "[80 rows x 108 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(healthy_features_path)\n",
    "data_clean = data.dropna()\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_healthy = data_clean.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 107)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_healthy.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_healthy.values.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_healthy = data_clean.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_healthy.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with NaN values:\n",
      "Empty DataFrame\n",
      "Columns: [1, 2, 3, 4, 5, 6, 7, 8, 4.1, Unnamed: 9, 8.1, Unnamed: 11, Unnamed: 12, Unnamed: 13, Unnamed: 14, Unnamed: 15, Unnamed: 16, Unnamed: 17, Unnamed: 18, Unnamed: 19, Unnamed: 20, Unnamed: 21, Unnamed: 22, Unnamed: 23, Unnamed: 24, Unnamed: 25, Unnamed: 26, Unnamed: 27, Unnamed: 28, Unnamed: 29, Unnamed: 30, Unnamed: 31, Unnamed: 32, Unnamed: 33, Unnamed: 34, Unnamed: 35, Unnamed: 36, Unnamed: 37, Unnamed: 38, Unnamed: 39, Unnamed: 40, Unnamed: 41, Unnamed: 42, Unnamed: 43, Unnamed: 44, Unnamed: 45, Unnamed: 46, Unnamed: 47, Unnamed: 48, Unnamed: 49, Unnamed: 50, Unnamed: 51, Unnamed: 52, Unnamed: 53, Unnamed: 54, Unnamed: 55, Unnamed: 56, Unnamed: 57, Unnamed: 58, Unnamed: 59, Unnamed: 60, Unnamed: 61, Unnamed: 62, Unnamed: 63, Unnamed: 64, Unnamed: 65, Unnamed: 66, Unnamed: 67, Unnamed: 68, Unnamed: 69, Unnamed: 70, Unnamed: 71, Unnamed: 72, Unnamed: 73, Unnamed: 74, Unnamed: 75, Unnamed: 76, Unnamed: 77, Unnamed: 78, Unnamed: 79, Unnamed: 80, Unnamed: 81, Unnamed: 82, Unnamed: 83, Unnamed: 84, Unnamed: 85, Unnamed: 86, Unnamed: 87, Unnamed: 88, Unnamed: 89, Unnamed: 90, Unnamed: 91, Unnamed: 92, Unnamed: 93, Unnamed: 94, Unnamed: 95, Unnamed: 96, Unnamed: 97, Unnamed: 98, Unnamed: 99, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 108 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check for rows with NaN values\n",
    "nan_rows = data[data.isnull().any(axis=1)]\n",
    "\n",
    "# Print rows with NaN values\n",
    "print(\"Rows with NaN values:\")\n",
    "print(nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>4.1</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 98</th>\n",
       "      <th>Unnamed: 99</th>\n",
       "      <th>Unnamed: 100</th>\n",
       "      <th>Unnamed: 101</th>\n",
       "      <th>Unnamed: 102</th>\n",
       "      <th>Unnamed: 103</th>\n",
       "      <th>Unnamed: 104</th>\n",
       "      <th>Unnamed: 105</th>\n",
       "      <th>Unnamed: 106</th>\n",
       "      <th>Unnamed: 107</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [1, 2, 3, 4, 5, 6, 7, 8, 4.1, Unnamed: 9, 8.1, Unnamed: 11, Unnamed: 12, Unnamed: 13, Unnamed: 14, Unnamed: 15, Unnamed: 16, Unnamed: 17, Unnamed: 18, Unnamed: 19, Unnamed: 20, Unnamed: 21, Unnamed: 22, Unnamed: 23, Unnamed: 24, Unnamed: 25, Unnamed: 26, Unnamed: 27, Unnamed: 28, Unnamed: 29, Unnamed: 30, Unnamed: 31, Unnamed: 32, Unnamed: 33, Unnamed: 34, Unnamed: 35, Unnamed: 36, Unnamed: 37, Unnamed: 38, Unnamed: 39, Unnamed: 40, Unnamed: 41, Unnamed: 42, Unnamed: 43, Unnamed: 44, Unnamed: 45, Unnamed: 46, Unnamed: 47, Unnamed: 48, Unnamed: 49, Unnamed: 50, Unnamed: 51, Unnamed: 52, Unnamed: 53, Unnamed: 54, Unnamed: 55, Unnamed: 56, Unnamed: 57, Unnamed: 58, Unnamed: 59, Unnamed: 60, Unnamed: 61, Unnamed: 62, Unnamed: 63, Unnamed: 64, Unnamed: 65, Unnamed: 66, Unnamed: 67, Unnamed: 68, Unnamed: 69, Unnamed: 70, Unnamed: 71, Unnamed: 72, Unnamed: 73, Unnamed: 74, Unnamed: 75, Unnamed: 76, Unnamed: 77, Unnamed: 78, Unnamed: 79, Unnamed: 80, Unnamed: 81, Unnamed: 82, Unnamed: 83, Unnamed: 84, Unnamed: 85, Unnamed: 86, Unnamed: 87, Unnamed: 88, Unnamed: 89, Unnamed: 90, Unnamed: 91, Unnamed: 92, Unnamed: 93, Unnamed: 94, Unnamed: 95, Unnamed: 96, Unnamed: 97, Unnamed: 98, Unnamed: 99, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 108 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of row values:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Define an empty list to store row values\n",
    "row_values_list = []\n",
    "\n",
    "# Iterate over the DataFrame to append values of each row into the list\n",
    "for index, row in nan_rows.iterrows():\n",
    "    row_values_list.append(row.tolist())\n",
    "\n",
    "# Print the list of row values\n",
    "print(\"List of row values:\")\n",
    "print(row_values_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "existing_array = np.array([[1, 2, 3],\n",
    "                           [4, 5, 6]])\n",
    "\n",
    "# New row to add\n",
    "new_row = np.array([7, 8, 9])\n",
    "\n",
    "# Adding a new row using np.vstack()\n",
    "modified_array = np.vstack((existing_array, new_row))\n",
    "print(modified_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 98</th>\n",
       "      <th>Unnamed: 99</th>\n",
       "      <th>Unnamed: 100</th>\n",
       "      <th>Unnamed: 101</th>\n",
       "      <th>Unnamed: 102</th>\n",
       "      <th>Unnamed: 103</th>\n",
       "      <th>Unnamed: 104</th>\n",
       "      <th>Unnamed: 105</th>\n",
       "      <th>Unnamed: 106</th>\n",
       "      <th>Unnamed: 107</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.319353</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>125.477002</td>\n",
       "      <td>44.687500</td>\n",
       "      <td>117.734375</td>\n",
       "      <td>135.064117</td>\n",
       "      <td>135.064117</td>\n",
       "      <td>2673.338826</td>\n",
       "      <td>40.071452</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015076</td>\n",
       "      <td>5.958638</td>\n",
       "      <td>0.112009</td>\n",
       "      <td>2523.268838</td>\n",
       "      <td>2.473413</td>\n",
       "      <td>0.003323</td>\n",
       "      <td>29.072921</td>\n",
       "      <td>0.014504</td>\n",
       "      <td>0.227486</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.268981</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>171.110519</td>\n",
       "      <td>51.562500</td>\n",
       "      <td>126.328125</td>\n",
       "      <td>189.462468</td>\n",
       "      <td>189.462468</td>\n",
       "      <td>3624.559530</td>\n",
       "      <td>46.025407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>5.979361</td>\n",
       "      <td>0.300394</td>\n",
       "      <td>63.402622</td>\n",
       "      <td>0.312355</td>\n",
       "      <td>0.002028</td>\n",
       "      <td>456.073789</td>\n",
       "      <td>0.023048</td>\n",
       "      <td>3.711866</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.316529</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>124.029475</td>\n",
       "      <td>42.109375</td>\n",
       "      <td>71.328125</td>\n",
       "      <td>115.872307</td>\n",
       "      <td>115.872307</td>\n",
       "      <td>2473.136902</td>\n",
       "      <td>39.258905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015853</td>\n",
       "      <td>6.106666</td>\n",
       "      <td>0.306395</td>\n",
       "      <td>35.450356</td>\n",
       "      <td>1.605275</td>\n",
       "      <td>0.004038</td>\n",
       "      <td>93.577571</td>\n",
       "      <td>0.059002</td>\n",
       "      <td>0.409372</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.372188</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>168.512073</td>\n",
       "      <td>52.421875</td>\n",
       "      <td>124.609375</td>\n",
       "      <td>153.086989</td>\n",
       "      <td>153.086989</td>\n",
       "      <td>3805.990601</td>\n",
       "      <td>62.718251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005313</td>\n",
       "      <td>6.274624</td>\n",
       "      <td>0.269012</td>\n",
       "      <td>71.571276</td>\n",
       "      <td>0.767073</td>\n",
       "      <td>0.002570</td>\n",
       "      <td>351.204857</td>\n",
       "      <td>0.012382</td>\n",
       "      <td>1.786180</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.393667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>134.186998</td>\n",
       "      <td>48.125000</td>\n",
       "      <td>128.046875</td>\n",
       "      <td>141.942633</td>\n",
       "      <td>141.942633</td>\n",
       "      <td>2849.723307</td>\n",
       "      <td>52.824930</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010695</td>\n",
       "      <td>5.609935</td>\n",
       "      <td>0.432405</td>\n",
       "      <td>7.162694</td>\n",
       "      <td>2.307508</td>\n",
       "      <td>0.002424</td>\n",
       "      <td>135.190550</td>\n",
       "      <td>0.061111</td>\n",
       "      <td>0.243941</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>0.238967</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>267.300895</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>148.671875</td>\n",
       "      <td>171.713792</td>\n",
       "      <td>171.713792</td>\n",
       "      <td>1465.234375</td>\n",
       "      <td>63.876119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031078</td>\n",
       "      <td>5.278934</td>\n",
       "      <td>0.121083</td>\n",
       "      <td>2009.603599</td>\n",
       "      <td>1.863040</td>\n",
       "      <td>0.004049</td>\n",
       "      <td>33.004101</td>\n",
       "      <td>0.011812</td>\n",
       "      <td>0.485635</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0.723967</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.692039</td>\n",
       "      <td>42.109375</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>55.428022</td>\n",
       "      <td>55.428022</td>\n",
       "      <td>884.384155</td>\n",
       "      <td>38.871242</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025740</td>\n",
       "      <td>5.475583</td>\n",
       "      <td>0.180159</td>\n",
       "      <td>136.511751</td>\n",
       "      <td>1.660235</td>\n",
       "      <td>0.007308</td>\n",
       "      <td>25.795424</td>\n",
       "      <td>0.028896</td>\n",
       "      <td>0.328948</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>0.311220</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>170.212424</td>\n",
       "      <td>48.984375</td>\n",
       "      <td>108.281250</td>\n",
       "      <td>182.806657</td>\n",
       "      <td>182.806657</td>\n",
       "      <td>2727.128092</td>\n",
       "      <td>52.973452</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025742</td>\n",
       "      <td>5.224807</td>\n",
       "      <td>0.119204</td>\n",
       "      <td>3490.677242</td>\n",
       "      <td>4.764778</td>\n",
       "      <td>0.002315</td>\n",
       "      <td>21.245111</td>\n",
       "      <td>0.012816</td>\n",
       "      <td>0.121341</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>0.273835</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>128.562779</td>\n",
       "      <td>34.375000</td>\n",
       "      <td>71.328125</td>\n",
       "      <td>121.549169</td>\n",
       "      <td>121.549169</td>\n",
       "      <td>1934.013367</td>\n",
       "      <td>35.204960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005154</td>\n",
       "      <td>6.180942</td>\n",
       "      <td>0.143015</td>\n",
       "      <td>850.538600</td>\n",
       "      <td>0.155308</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>368.726573</td>\n",
       "      <td>0.013234</td>\n",
       "      <td>10.172534</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>0.257815</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>161.593961</td>\n",
       "      <td>35.234375</td>\n",
       "      <td>92.812500</td>\n",
       "      <td>141.233255</td>\n",
       "      <td>141.233255</td>\n",
       "      <td>2173.110962</td>\n",
       "      <td>41.661292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032693</td>\n",
       "      <td>5.681340</td>\n",
       "      <td>0.271682</td>\n",
       "      <td>132.751783</td>\n",
       "      <td>2.036386</td>\n",
       "      <td>0.003247</td>\n",
       "      <td>75.952881</td>\n",
       "      <td>0.043651</td>\n",
       "      <td>0.305322</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>281 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            1  2  3           4           5           6           7  \\\n",
       "0    0.319353  0  0  125.477002   44.687500  117.734375  135.064117   \n",
       "1    0.268981  0  0  171.110519   51.562500  126.328125  189.462468   \n",
       "2    0.316529  0  0  124.029475   42.109375   71.328125  115.872307   \n",
       "3    0.372188  0  0  168.512073   52.421875  124.609375  153.086989   \n",
       "4    0.393667  0  0  134.186998   48.125000  128.046875  141.942633   \n",
       "..        ... .. ..         ...         ...         ...         ...   \n",
       "276  0.238967  0  0  267.300895  110.000000  148.671875  171.713792   \n",
       "277  0.723967  0  0   53.692039   42.109375   55.000000   55.428022   \n",
       "278  0.311220  0  0  170.212424   48.984375  108.281250  182.806657   \n",
       "279  0.273835  0  0  128.562779   34.375000   71.328125  121.549169   \n",
       "280  0.257815  0  0  161.593961   35.234375   92.812500  141.233255   \n",
       "\n",
       "              8            9         10  ...  Unnamed: 98  Unnamed: 99  \\\n",
       "0    135.064117  2673.338826  40.071452  ...     0.015076     5.958638   \n",
       "1    189.462468  3624.559530  46.025407  ...     0.001082     5.979361   \n",
       "2    115.872307  2473.136902  39.258905  ...     0.015853     6.106666   \n",
       "3    153.086989  3805.990601  62.718251  ...     0.005313     6.274624   \n",
       "4    141.942633  2849.723307  52.824930  ...     0.010695     5.609935   \n",
       "..          ...          ...        ...  ...          ...          ...   \n",
       "276  171.713792  1465.234375  63.876119  ...     0.031078     5.278934   \n",
       "277   55.428022   884.384155  38.871242  ...     0.025740     5.475583   \n",
       "278  182.806657  2727.128092  52.973452  ...     0.025742     5.224807   \n",
       "279  121.549169  1934.013367  35.204960  ...     0.005154     6.180942   \n",
       "280  141.233255  2173.110962  41.661292  ...     0.032693     5.681340   \n",
       "\n",
       "     Unnamed: 100  Unnamed: 101  Unnamed: 102  Unnamed: 103  Unnamed: 104  \\\n",
       "0        0.112009   2523.268838      2.473413      0.003323     29.072921   \n",
       "1        0.300394     63.402622      0.312355      0.002028    456.073789   \n",
       "2        0.306395     35.450356      1.605275      0.004038     93.577571   \n",
       "3        0.269012     71.571276      0.767073      0.002570    351.204857   \n",
       "4        0.432405      7.162694      2.307508      0.002424    135.190550   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "276      0.121083   2009.603599      1.863040      0.004049     33.004101   \n",
       "277      0.180159    136.511751      1.660235      0.007308     25.795424   \n",
       "278      0.119204   3490.677242      4.764778      0.002315     21.245111   \n",
       "279      0.143015    850.538600      0.155308      0.004182    368.726573   \n",
       "280      0.271682    132.751783      2.036386      0.003247     75.952881   \n",
       "\n",
       "     Unnamed: 105  Unnamed: 106  Unnamed: 107  \n",
       "0        0.014504      0.227486             1  \n",
       "1        0.023048      3.711866             1  \n",
       "2        0.059002      0.409372             1  \n",
       "3        0.012382      1.786180             1  \n",
       "4        0.061111      0.243941             1  \n",
       "..            ...           ...           ...  \n",
       "276      0.011812      0.485635             1  \n",
       "277      0.028896      0.328948             1  \n",
       "278      0.012816      0.121341             1  \n",
       "279      0.013234     10.172534             1  \n",
       "280      0.043651      0.305322             1  \n",
       "\n",
       "[281 rows x 108 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tumor = pd.read_csv(tumor_features_path)\n",
    "data_tumor_clean = data_tumor.dropna()\n",
    "data_tumor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tumor = data_tumor_clean.iloc[:, :-1]\n",
    "y_tumor = data_tumor_clean.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with NaN values:\n",
      "Empty DataFrame\n",
      "Columns: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, Unnamed: 10, Unnamed: 11, Unnamed: 12, Unnamed: 13, Unnamed: 14, Unnamed: 15, Unnamed: 16, Unnamed: 17, Unnamed: 18, Unnamed: 19, Unnamed: 20, Unnamed: 21, Unnamed: 22, Unnamed: 23, Unnamed: 24, Unnamed: 25, Unnamed: 26, Unnamed: 27, Unnamed: 28, Unnamed: 29, Unnamed: 30, Unnamed: 31, Unnamed: 32, Unnamed: 33, Unnamed: 34, Unnamed: 35, Unnamed: 36, Unnamed: 37, Unnamed: 38, Unnamed: 39, Unnamed: 40, Unnamed: 41, Unnamed: 42, Unnamed: 43, Unnamed: 44, Unnamed: 45, Unnamed: 46, Unnamed: 47, Unnamed: 48, Unnamed: 49, Unnamed: 50, Unnamed: 51, Unnamed: 52, Unnamed: 53, Unnamed: 54, Unnamed: 55, Unnamed: 56, Unnamed: 57, Unnamed: 58, Unnamed: 59, Unnamed: 60, Unnamed: 61, Unnamed: 62, Unnamed: 63, Unnamed: 64, Unnamed: 65, Unnamed: 66, Unnamed: 67, Unnamed: 68, Unnamed: 69, Unnamed: 70, Unnamed: 71, Unnamed: 72, Unnamed: 73, Unnamed: 74, Unnamed: 75, Unnamed: 76, Unnamed: 77, Unnamed: 78, Unnamed: 79, Unnamed: 80, Unnamed: 81, Unnamed: 82, Unnamed: 83, Unnamed: 84, Unnamed: 85, Unnamed: 86, Unnamed: 87, Unnamed: 88, Unnamed: 89, Unnamed: 90, Unnamed: 91, Unnamed: 92, Unnamed: 93, Unnamed: 94, Unnamed: 95, Unnamed: 96, Unnamed: 97, Unnamed: 98, Unnamed: 99, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 108 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check for rows with NaN values\n",
    "nan_rows = data_tumor[data_tumor.isnull().any(axis=1)]\n",
    "\n",
    "# Print rows with NaN values\n",
    "print(\"Rows with NaN values:\")\n",
    "print(nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(281, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=y_tumor.values.reshape(281,1)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = y_healthy.values.reshape(80,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0]]\n"
     ]
    }
   ],
   "source": [
    "modified_array = np.vstack((z, h))\n",
    "y = modified_array.reshape(1,361)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.19352960e-01  0.00000000e+00  0.00000000e+00 ...  8.03799466e-01\n",
      "   1.15677630e+00  1.94830767e+00]\n",
      " [ 1.40132850e+00  5.87909771e-01  5.48909034e-01 ...  2.43103113e-02\n",
      "   4.32686259e+00  1.43741988e+03]\n",
      " [ 4.88279424e-01  6.36809950e-01  1.10297777e+00 ...  1.61092895e-01\n",
      "   5.96007379e+03  2.13052085e+00]\n",
      " ...\n",
      " [ 3.71331023e+02  3.21777316e-01  5.89020182e-01 ... -1.56000000e+02\n",
      "   3.12000000e+02  2.41249031e+01]\n",
      " [ 7.42834603e+01 -1.05425063e+00  1.09174798e+07 ...  1.26571925e+03\n",
      "   1.77893289e-01  1.62035640e-02]\n",
      " [ 2.61464344e-01  3.95397679e+01  4.92180800e-03 ...  1.17422719e+03\n",
      "   8.33567720e-02  1.50202582e+01]]\n"
     ]
    }
   ],
   "source": [
    "z1=X_tumor.values.reshape(281,107)\n",
    "h1 = X_healthy.values.reshape(80,107)\n",
    "modified_array = np.vstack((z1, h1))\n",
    "X = modified_array.reshape(107,361)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(107, 361)\n",
      "(1, 361)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "X =X.reshape(361,107)\n",
    "print(y.shape)\n",
    "y = y.reshape(361,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.27485932e-01, 3.71186617e+00, 4.09371981e-01, 1.78617992e+00,\n",
       "       2.43941079e-01, 1.52145484e-01, 1.02113650e+01, 2.79512555e-01,\n",
       "       7.80697598e-01, 2.86348951e-01, 3.06085934e-01, 2.68229824e-01,\n",
       "       6.87065711e+00, 3.57418345e+01, 2.54794096e+01, 2.79645533e-01,\n",
       "       5.05088101e-01, 2.36013059e-01, 1.72992757e-01, 2.58451493e+01,\n",
       "       1.03911005e+00, 5.80695463e+00, 3.14386827e-01, 5.84248416e-01,\n",
       "       1.98037487e-01, 1.74206563e-01, 1.46004787e+01, 3.53578304e-01,\n",
       "       1.91979379e-01, 3.13365392e+00, 2.65882711e-01, 8.52964458e-01,\n",
       "       3.58428687e-01, 3.26595297e-01, 4.20677677e-01, 2.29331235e-01,\n",
       "       4.55782739e-01, 4.57466074e-01, 2.85013495e-01, 3.18062368e-01,\n",
       "       7.52759996e-01, 5.86315354e-01, 3.16578574e-01, 3.13060427e-01,\n",
       "       5.91211834e-01, 1.71051907e-01, 5.50310788e+00, 3.24146598e-01,\n",
       "       2.31627857e+00, 1.66756178e-01, 4.23177400e+01, 1.83526239e-01,\n",
       "       2.58823036e-01, 2.47741873e-01, 1.26437715e-01, 1.51812269e-01,\n",
       "       2.49828515e-01, 9.00662824e-01, 1.47973095e-01, 1.83131019e-01,\n",
       "       1.54840961e-01, 2.74995124e-01, 1.06632937e+00, 2.88366985e-01,\n",
       "       2.05862278e-01, 1.32837131e-01, 3.58008144e-01, 2.73117725e-01,\n",
       "       2.92211269e+01, 2.09845497e-01, 1.10021958e+01, 1.67392541e+00,\n",
       "       1.33514437e-01, 3.21335426e-01, 3.42067597e-01, 5.36621054e+00,\n",
       "       7.27381580e-01, 3.72031064e-01, 1.70828556e+00, 1.52462544e-01,\n",
       "       2.28789708e-01, 2.55940446e-01, 7.15139513e+00, 1.40770749e-01,\n",
       "       2.08652336e-01, 1.13416343e+01, 1.65572566e-01, 2.75815628e+01,\n",
       "       2.78799525e-01, 2.55291948e-01, 6.48354549e+00, 3.61746396e-01,\n",
       "       2.05694984e-01, 1.61881923e-01, 2.29675094e-01, 5.17913740e-01,\n",
       "       1.02082675e+00, 4.44907422e-01, 2.29047927e-01, 7.16855688e+01,\n",
       "       9.41837974e+01, 1.95577783e-01, 1.29857387e+00, 3.59072678e-01,\n",
       "       6.77085740e-01, 3.93255870e-01, 6.87701546e-01, 4.19353904e-01,\n",
       "       3.49990610e-01, 7.00893432e-01, 5.57120939e-02, 4.93476049e-01,\n",
       "       5.51051358e-01, 2.18976112e-01, 6.09911620e+00, 3.04439133e-01,\n",
       "       3.94415672e-01, 2.19599814e-01, 1.36681929e-01, 3.08385364e-01,\n",
       "       3.35497146e-01, 2.25830700e-01, 2.60634817e+02, 4.04731037e-01,\n",
       "       3.87663986e-01, 4.05093153e-01, 1.31117183e+01, 2.90384384e-01,\n",
       "       5.12751846e-01, 4.49826665e-01, 3.31452089e-01, 1.48896268e+00,\n",
       "       1.66767990e+01, 4.69074197e-01, 7.28340366e+01, 2.87323034e-01,\n",
       "       2.83893169e-01, 2.48508718e+01, 6.50716184e-01, 5.31753825e-01,\n",
       "       4.58150541e-01, 3.13180234e+01, 4.39898206e-01, 1.55445302e-01,\n",
       "       1.54145091e-01, 7.36463916e+01, 1.77489500e-01, 5.28958617e-01,\n",
       "       3.53385574e-01, 1.42767234e-01, 1.13074396e+00, 2.49218652e-01,\n",
       "       2.17081652e-01, 1.40601168e+01, 3.01965284e-01, 4.98013934e-01,\n",
       "       6.68673993e-01, 1.66391365e+00, 1.89929337e-01, 3.33721126e-01,\n",
       "       3.07552004e-01, 6.84855531e-01, 3.42658332e-01, 5.23194950e-01,\n",
       "       2.44088444e-01, 2.25190236e-01, 3.45946664e-01, 2.06205962e-01,\n",
       "       2.47234640e-01, 1.87169122e+01, 1.36992980e+01, 3.15045003e+01,\n",
       "       7.31693363e+00, 3.49210920e-01, 5.01856774e-01, 3.11542649e-01,\n",
       "       3.13456753e+01, 4.09802347e-01, 1.96570673e-01, 1.51817651e-01,\n",
       "       2.98511543e+01, 9.34943890e-01, 9.14778004e-01, 7.25912493e-01,\n",
       "       3.93425455e-01, 2.41469330e-01, 4.68849917e-01, 4.73639321e-01,\n",
       "       1.84366714e-01, 2.54156398e-01, 3.38345320e-01, 1.99770078e-01,\n",
       "       3.68901992e-01, 4.59192685e-01, 3.61579511e-01, 2.62110311e-01,\n",
       "       8.20907063e-01, 4.40765247e-01, 1.61345292e+01, 1.57234156e-01,\n",
       "       4.64447718e-01, 3.50316362e-01, 3.86818772e-01, 3.61445765e-01,\n",
       "       2.23771517e-01, 5.20335541e-01, 2.84090292e-01, 7.83102990e-02,\n",
       "       2.05309694e-01, 2.18479440e-01, 1.23192255e-01, 2.28333226e-01,\n",
       "       2.31587859e+00, 2.27771158e-01, 6.50325430e-02, 5.24282286e-01,\n",
       "       8.95558271e-01, 2.57717108e-01, 2.27186765e-01, 3.48417921e-01,\n",
       "       2.26829057e-01, 2.56254765e-01, 1.06899801e+00, 4.37111273e-01,\n",
       "       2.00619259e-01, 1.01676512e+00, 1.65767786e-01, 4.81171001e-01,\n",
       "       3.06637982e-01, 8.96286877e+01, 5.65027310e-01, 1.65599389e-01,\n",
       "       1.69507644e-01, 1.19225115e-01, 1.54043079e-01, 1.00326491e+00,\n",
       "       1.85019113e-01, 8.98557760e-02, 2.42160225e-01, 2.42541157e-01,\n",
       "       2.94373782e-01, 3.26913345e-01, 1.55064544e-01, 1.66636062e-01,\n",
       "       1.48826779e+00, 3.23143429e-01, 2.12327013e-01, 2.94013268e-01,\n",
       "       1.35367283e-01, 2.28963708e-01, 2.52423563e-01, 4.14743264e-01,\n",
       "       3.41712655e-01, 5.72439864e-01, 1.74983947e-01, 2.41988482e-01,\n",
       "       1.84409676e+01, 1.87278305e-01, 3.59858867e-01, 2.07582492e-01,\n",
       "       2.36221567e-01, 2.02989487e-01, 6.31034154e-01, 3.16534065e-01,\n",
       "       3.29478435e-01, 3.85294729e-01, 1.02051388e-01, 3.28849624e-01,\n",
       "       2.13204533e-01, 1.64448429e-01, 1.21291965e-01, 2.08312842e-01,\n",
       "       5.21119742e+00, 3.99064557e-01, 1.56188141e-01, 7.62870062e-01,\n",
       "       4.85635430e-01, 3.28948333e-01, 1.21341414e-01, 1.01725344e+01,\n",
       "       3.05321657e-01, 2.20057336e-01, 2.25208756e-01, 2.09898918e+01,\n",
       "       5.02555348e-01, 5.35531944e-01, 1.32637742e+00, 8.95272063e-01,\n",
       "       4.56896095e-01, 4.88557403e-01, 5.11288905e-01, 4.98962961e-01,\n",
       "       1.51777397e+00, 1.65155736e+01, 7.79883108e-01, 2.27867575e+01,\n",
       "       2.22197656e-01, 5.29335523e+00, 5.57562889e-01, 8.64231554e-01,\n",
       "       3.28105478e-01, 2.70235201e-01, 2.66911869e+01, 3.45613230e+00,\n",
       "       1.43462378e-01, 3.02475892e+01, 5.72449661e-01, 2.58940356e+01,\n",
       "       4.39739411e-01, 5.05359864e-01, 5.21961121e-01, 1.65189928e+01,\n",
       "       1.09687975e+00, 2.70043290e-01, 1.66083561e+01, 1.22041398e+01,\n",
       "       1.36619786e+01, 4.74542509e-01, 1.34884503e+01, 9.13913213e-01,\n",
       "       1.76999545e+01, 1.56980590e+01, 5.82397827e-01, 5.96272582e-01,\n",
       "       1.56393570e+01, 1.40572859e+01, 6.86886028e-01, 5.12919762e-01,\n",
       "       1.10202582e+01, 1.01377275e+01, 1.85566449e+00, 1.72546190e+00,\n",
       "       1.31987597e-01, 1.60904178e+01, 7.53381038e-01, 7.38504755e-01,\n",
       "       1.44520522e+01, 4.29727968e-01, 1.15512852e+01, 1.60781131e+00,\n",
       "       1.06549176e+00, 2.98703468e+00, 8.10357144e-01, 9.49068259e+00,\n",
       "       1.41625742e+01, 1.36211003e+00, 8.93942214e-01, 2.06680997e-01,\n",
       "       3.09944398e-01, 1.03646977e+00, 1.43748024e+01, 4.49011718e-01,\n",
       "       5.59031610e-01, 4.68516799e+00, 2.78749535e-01, 1.93749342e+01,\n",
       "       5.01310826e-01, 4.05527252e+00, 3.73231658e+00, 9.38922215e-01,\n",
       "       1.50202582e+01])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(361, 107)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.21800605e-01,  0.00000000e+00,  0.00000000e+00,  1.41648798e+02,\n",
       "        4.12500000e+01,  6.70312500e+01,  1.24609375e+02,  1.24609375e+02,\n",
       "        2.22831573e+03,  4.55826690e+01,  1.70751660e-01,  4.83174130e+03,\n",
       "        2.16833783e+00,  2.30789185e+03,  2.10000000e+01,  1.24000000e+02,\n",
       "        2.41136530e+07,  2.68416678e+00,  5.60000000e+01,  2.62375319e+00,\n",
       "        1.87000000e+02,  3.22862340e+01,  7.85059200e+01,  8.40000000e+01,\n",
       "       -6.80000000e+01,  2.55000000e+02,  2.36637479e+01,  8.78428652e+01,\n",
       "       -3.53242480e-01,  1.78085450e+07,  1.78891059e-01,  1.55318949e+03,\n",
       "        4.62738319e+01,  1.89139658e+02, -1.05707991e+01,  8.54330955e+00,\n",
       "        1.40635467e+00,  7.17141875e-01,  8.59171444e-01,  1.62719677e+00,\n",
       "        6.56003831e-01,  6.47677932e-01,  6.23903071e-01,  9.88835877e-01,\n",
       "        9.31742877e-01, -2.03866433e-01,  8.03906221e-01,  5.03379428e-01,\n",
       "        6.67004849e+00,  5.07310840e-02,  4.76670761e+00,  7.35787255e-01,\n",
       "        1.04518936e-01,  1.33400970e+01,  3.51768613e+00,  2.48741605e+00,\n",
       "        5.34614055e+00,  5.21875520e+02,  1.67000166e-01,  2.79508111e+00,\n",
       "        5.59034560e+02,  2.57308406e+00,  4.68796800e+01,  1.66192000e+01,\n",
       "        7.98489920e+02,  4.41395060e-01,  2.90864161e-02,  1.72228047e-01,\n",
       "        7.89608405e+00,  5.93303000e-03,  3.51664156e+02,  1.70457948e-01,\n",
       "        2.72944168e+00,  4.64207872e+01,  3.18103597e+00,  1.51213814e+02,\n",
       "        8.80528190e-02,  3.02021830e-02,  4.10840545e+00,  1.04135420e+03,\n",
       "        5.01879831e-01,  6.60240000e-01,  8.57983705e-01,  7.32871639e-01,\n",
       "        3.37400615e+01,  2.28716600e-02,  9.21172638e+01,  1.50028117e-01,\n",
       "        3.44848221e+00,  4.55081433e+01,  1.50066775e+02,  7.57288111e+03,\n",
       "        3.75377308e+00,  3.50880830e-02,  1.34996743e+02,  2.19864402e-01,\n",
       "        4.70888792e-01,  2.15161737e+01,  1.82766880e-02,  5.70362617e+00,\n",
       "        1.96480000e-01,  1.24162986e+02,  3.47859343e+00,  3.37904000e-03,\n",
       "        3.06692730e+01,  2.77847100e-02,  1.54145091e-01])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(361, 1)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y=y.reshape((361,1))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_test\n",
    "count = np.sum(y_test == 1)\n",
    "count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "steps = list()\n",
    "steps.append(('scaler',MinMaxScaler()))\n",
    "steps.append(('model',rf_classifier))\n",
    "pipeline = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train) #computes the mean and std\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs = rf_classifier.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "# print(len(y_pred))\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_probs)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Calculate sensitivity (recall) and specificity\n",
    "tn, fp, fn, tp = confusion_mat.ravel()\n",
    "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "metrics_dict = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Sensitivity (Recall)': sensitivity,\n",
    "    'Specificity': specificity,\n",
    "    'Confusion Matrix': confusion_mat,\n",
    "    'Classification Report': classification_rep,\n",
    "    'ROC AUC': roc_auc,\n",
    "    'F1 Score': f1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(confusion_mat, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "\n",
    "# Add label annotations\n",
    "classes = ['Class 0', 'Class 1']\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "thresh = confusion_mat.max() / 2.\n",
    "for i in range(confusion_mat.shape[0]):\n",
    "    for j in range(confusion_mat.shape[1]):\n",
    "        plt.text(j, i, format(confusion_mat[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if confusion_mat[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.9726027397260274,\n",
       " 'Sensitivity (Recall)': 1.0,\n",
       " 'Specificity': 0.875,\n",
       " 'Confusion Matrix': array([[14,  2],\n",
       "        [ 0, 57]]),\n",
       " 'Classification Report': '              precision    recall  f1-score   support\\n\\n           0       1.00      0.88      0.93        16\\n           1       0.97      1.00      0.98        57\\n\\n    accuracy                           0.97        73\\n   macro avg       0.98      0.94      0.96        73\\nweighted avg       0.97      0.97      0.97        73\\n',\n",
       " 'ROC AUC': 0.9375,\n",
       " 'F1 Score': 0.9827586206896551}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train) #computes the mean and std\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier = XGBClassifier(n_estimators=100, learning_rate=0.01, random_state=42)\n",
    "\n",
    "steps = list()\n",
    "steps.append(('scaler',MinMaxScaler()))\n",
    "steps.append(('model',xgb_classifier))\n",
    "pipeline = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=42, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=42, ...)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#XGboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_classifier = XGBClassifier(n_estimators=100, learning_rate=0.01, random_state=42)\n",
    "xgb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs = xgb_classifier.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_probs)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Calculate sensitivity (recall) and specificity\n",
    "tn, fp, fn, tp = confusion_mat.ravel()\n",
    "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "metrics_dict = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Sensitivity (Recall)': sensitivity,\n",
    "    'Specificity': specificity,\n",
    "    'Confusion Matrix': confusion_mat,\n",
    "    'Classification Report': classification_rep,\n",
    "    'ROC AUC': roc_auc,\n",
    "    'F1 Score': f1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.9863013698630136,\n",
       " 'Sensitivity (Recall)': 1.0,\n",
       " 'Specificity': 0.9375,\n",
       " 'Confusion Matrix': array([[15,  1],\n",
       "        [ 0, 57]]),\n",
       " 'Classification Report': '              precision    recall  f1-score   support\\n\\n           0       1.00      0.94      0.97        16\\n           1       0.98      1.00      0.99        57\\n\\n    accuracy                           0.99        73\\n   macro avg       0.99      0.97      0.98        73\\nweighted avg       0.99      0.99      0.99        73\\n',\n",
       " 'ROC AUC': 0.96875,\n",
       " 'F1 Score': 0.9913043478260869}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.97260274 0.94444444 0.90277778 0.88888889 0.95833333]\n",
      "Mean accuracy: 0.9334094368340944\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation and compute scores\n",
    "scores = cross_val_score(pipeline, X, y, cv=kfold)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean accuracy:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 0.9306363304408738\n"
     ]
    }
   ],
   "source": [
    "precision_scores = cross_val_score(rf_classifier, X, y, cv=kfold, scoring='precision')\n",
    "\n",
    "# Calculate average precision\n",
    "average_precision = np.mean(precision_scores)\n",
    "print(\"Average Precision:\", average_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROC AUC: 0.9926494380240725\n"
     ]
    }
   ],
   "source": [
    "roc_auc_scores = cross_val_score(rf_classifier, X, y, cv=kfold, scoring='roc_auc')\n",
    "\n",
    "# Calculate average precision\n",
    "average_roc_auc = np.mean(roc_auc_scores)\n",
    "print(\"Average ROC AUC:\", average_roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Recall: 0.9894110275689222\n"
     ]
    }
   ],
   "source": [
    "recall_scores = cross_val_score(rf_classifier, X, y, cv=kfold, scoring='recall')\n",
    "\n",
    "# Calculate average precision\n",
    "average_recall = np.mean(recall_scores)\n",
    "print(\"Average Recall:\", average_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F-1 Score: 0.9588923303915807\n"
     ]
    }
   ],
   "source": [
    "f1_scores = cross_val_score(rf_classifier, X, y, cv=kfold, scoring='f1')\n",
    "\n",
    "# Calculate average precision\n",
    "average_f1 = np.mean(f1_scores)\n",
    "print(\"Average F-1 Score:\", average_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.98275862 0.93103448 0.94915254 0.93442623 0.96491228]\n",
      "Mean precision: 0.9524568312062216\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation and compute scores\n",
    "scores = cross_val_score(xgb_classifier, X, y, cv=kfold,scoring = 'precision')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean precision:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [1.         0.96193416 0.98772321 0.99883041 0.99181287]\n",
      "Mean ROC AUC: 0.9880601291036234\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation and compute scores\n",
    "scores = cross_val_score(xgb_classifier, X, y, cv=kfold,scoring = 'roc_auc')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean ROC AUC:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.98630137 0.94444444 0.95833333 0.94444444 0.94444444]\n",
      "Mean accuracy: 0.955593607305936\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation and compute scores\n",
    "scores = cross_val_score(pipeline, X, y, cv=kfold)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean accuracy:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [1.         1.         1.         1.         0.96491228]\n",
      "Mean recall: 0.9929824561403509\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation and compute scores\n",
    "scores = cross_val_score(pipeline, X, y, cv=kfold,scoring = \"recall\")\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean recall:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.99130435 0.96428571 0.97391304 0.96610169 0.96491228]\n",
      "Mean f1: 0.9721034162414142\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation and compute scores\n",
    "scores = cross_val_score(xgb_classifier, X, y, cv=kfold,scoring = \"f1\")\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean f1:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train) #computes the mean and std\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_classifier = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svc_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs = svc_classifier.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_probs)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Calculate sensitivity (recall) and specificity\n",
    "tp, fn, fp, tn = confusion_mat.ravel()\n",
    "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "metrics_dict = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Sensitivity (Recall)': sensitivity,\n",
    "    'Specificity': specificity,\n",
    "    'Confusion Matrix': confusion_mat,\n",
    "    'Classification Report': classification_rep,\n",
    "    'ROC AUC': roc_auc,\n",
    "    'F1 Score': f1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.9452054794520548,\n",
       " 'Sensitivity (Recall)': 0.75,\n",
       " 'Specificity': 1.0,\n",
       " 'Confusion Matrix': array([[12,  4],\n",
       "        [ 0, 57]]),\n",
       " 'Classification Report': '              precision    recall  f1-score   support\\n\\n           0       1.00      0.75      0.86        16\\n           1       0.93      1.00      0.97        57\\n\\n    accuracy                           0.95        73\\n   macro avg       0.97      0.88      0.91        73\\nweighted avg       0.95      0.95      0.94        73\\n',\n",
       " 'ROC AUC': 0.875,\n",
       " 'F1 Score': 0.9661016949152543}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_classifier = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "\n",
    "steps = list()\n",
    "steps.append((\"scaler\",MinMaxScaler()))\n",
    "steps.append((\"model\",svc_classifier))\n",
    "pipeline = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Cross-validation scores: [0.94520548 0.97222222 0.91666667 0.90277778 0.93055556]\n",
      "Mean accuracy: 0.9334855403348554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(type(X))\n",
    "# Perform k-fold cross-validation and compute scores\n",
    "scores = cross_val_score(pipeline, X, y, cv=kfold)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean accuracy:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.93442623 0.96428571 0.90322581 0.890625   0.91935484]\n",
      "Mean precision: 0.9223835177910402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation and compute scores\n",
    "scores = cross_val_score(pipeline, X, y, cv=kfold, scoring = \"precision\")\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean precision:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape\n",
    "y=y.reshape((361,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.99890351 0.98148148 0.98214286 0.95204678 0.98479532]\n",
      "Mean ROC AUC: 0.9798739905318854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation and compute scores\n",
    "scores = cross_val_score(pipeline, X, y, cv=kfold, scoring = \"roc_auc\")\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean ROC AUC:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [1. 1. 1. 1. 1.]\n",
      "Mean Recall: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation and compute scores\n",
    "scores = cross_val_score(pipeline, X, y, cv=kfold, scoring = \"recall\")\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean Recall:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.88372093 0.89256198 0.88888889 0.9047619  0.91935484]\n",
      "Mean F1: 0.8978577092128207\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation and compute scores\n",
    "scores = cross_val_score(svc_classifier, X, y, cv=kfold, scoring = \"f1\")\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean F1:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train) #computes the mean and std\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:233: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "# Train the classifier\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs = knn.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_probs)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Calculate sensitivity (recall) and specificity\n",
    "tn, fp, fn, tp = confusion_mat.ravel()\n",
    "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "metrics_dict = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Sensitivity (Recall)': sensitivity,\n",
    "    'Specificity': specificity,\n",
    "    'Confusion Matrix': confusion_mat,\n",
    "    'Classification Report': classification_rep,\n",
    "    'ROC AUC': roc_auc,\n",
    "    'F1 Score': f1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.9041095890410958,\n",
       " 'Sensitivity (Recall)': 0.9298245614035088,\n",
       " 'Specificity': 0.8125,\n",
       " 'Confusion Matrix': array([[13,  3],\n",
       "        [ 4, 53]]),\n",
       " 'Classification Report': '              precision    recall  f1-score   support\\n\\n           0       0.76      0.81      0.79        16\\n           1       0.95      0.93      0.94        57\\n\\n    accuracy                           0.90        73\\n   macro avg       0.86      0.87      0.86        73\\nweighted avg       0.91      0.90      0.91        73\\n',\n",
       " 'ROC AUC': 0.8711622807017544,\n",
       " 'F1 Score': 0.9380530973451328}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.94827586 0.98148148 0.85245902 0.89285714 0.93103448]\n",
      "Mean precision: 0.9212215971119306\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation and compute scores\n",
    "scores = cross_val_score(knn, X, y, cv=kfold,scoring = 'precision')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean precision:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.89967105 0.9686214  0.69196429 0.75497076 0.85789474]\n",
      "Mean ROC AUC: 0.8346244469197686\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation and compute scores\n",
    "scores = cross_val_score(knn, X, y, cv=kfold,scoring = 'roc_auc')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean ROC AUC:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.93150685 0.97222222 0.81944444 0.81944444 0.90277778]\n",
      "Mean accuracy: 0.8890791476407914\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation and compute scores\n",
    "scores = cross_val_score(knn, X, y, cv=kfold)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean accuracy:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.96491228 0.98148148 0.92857143 0.87719298 0.94736842]\n",
      "Mean recall: 0.9399053188526872\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation and compute scores\n",
    "scores = cross_val_score(knn, X, y, cv=kfold,scoring = \"recall\")\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean recall:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.95652174 0.98148148 0.88888889 0.88495575 0.93913043]\n",
      "Mean f1: 0.9301956592991607\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation and compute scores\n",
    "scores = cross_val_score(knn, X, y, cv=kfold,scoring = \"f1\")\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean f1:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train) #computes the mean and std\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Initialize the Gaussian Naive Bayes classifier\n",
    "naive_bayes = GaussianNB()\n",
    "\n",
    "# Train the classifier\n",
    "naive_bayes.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs = naive_bayes.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_probs)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Calculate sensitivity (recall) and specificity\n",
    "tn, fp, fn, tp = confusion_mat.ravel()\n",
    "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "metrics_dict = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Sensitivity (Recall)': sensitivity,\n",
    "    'Specificity': specificity,\n",
    "    'Confusion Matrix': confusion_mat,\n",
    "    'Classification Report': classification_rep,\n",
    "    'ROC AUC': roc_auc,\n",
    "    'F1 Score': f1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.9178082191780822,\n",
       " 'Sensitivity (Recall)': 0.9824561403508771,\n",
       " 'Specificity': 0.6875,\n",
       " 'Confusion Matrix': array([[11,  5],\n",
       "        [ 1, 56]]),\n",
       " 'Classification Report': '              precision    recall  f1-score   support\\n\\n           0       0.92      0.69      0.79        16\\n           1       0.92      0.98      0.95        57\\n\\n    accuracy                           0.92        73\\n   macro avg       0.92      0.83      0.87        73\\nweighted avg       0.92      0.92      0.91        73\\n',\n",
       " 'ROC AUC': 0.8349780701754386,\n",
       " 'F1 Score': 0.9491525423728813}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.79452055 0.83333333 0.80555556 0.80555556 0.84722222]\n",
      "Mean accuracy: 0.8172374429223744\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation and compute scores\n",
    "scores = cross_val_score(naive_bayes, X, y, cv=kfold)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean accuracy:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train) #computes the mean and std\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostClassifier(random_state=42)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# Initialize the AdaBoost classifier with DecisionTreeClassifier as the base estimator\n",
    "adaboost = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "adaboost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs = adaboost.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_probs)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Calculate sensitivity (recall) and specificity\n",
    "tn, fp, fn, tp = confusion_mat.ravel()\n",
    "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "metrics_dict = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Sensitivity (Recall)': sensitivity,\n",
    "    'Specificity': specificity,\n",
    "    'Confusion Matrix': confusion_mat,\n",
    "    'Classification Report': classification_rep,\n",
    "    'ROC AUC': roc_auc,\n",
    "    'F1 Score': f1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 1.0,\n",
       " 'Sensitivity (Recall)': 1.0,\n",
       " 'Specificity': 1.0,\n",
       " 'Confusion Matrix': array([[16,  0],\n",
       "        [ 0, 57]]),\n",
       " 'Classification Report': '              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00        16\\n           1       1.00      1.00      1.00        57\\n\\n    accuracy                           1.00        73\\n   macro avg       1.00      1.00      1.00        73\\nweighted avg       1.00      1.00      1.00        73\\n',\n",
       " 'ROC AUC': 1.0,\n",
       " 'F1 Score': 1.0}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [1.         0.97222222 0.95833333 0.90277778 0.97222222]\n",
      "Mean accuracy: 0.961111111111111\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation and compute scores\n",
    "scores = cross_val_score(adaboost, X, y, cv=kfold)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean accuracy:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train) #computes the mean and std\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(random_state=42)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Initialize the Decision Tree classifier\n",
    "decision_tree = DecisionTreeClassifier(criterion = \"gini\",random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "decision_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs = decision_tree.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_probs)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Calculate sensitivity (recall) and specificity\n",
    "tn, fp, fn, tp = confusion_mat.ravel()\n",
    "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "metrics_dict = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Sensitivity (Recall)': sensitivity,\n",
    "    'Specificity': specificity,\n",
    "    'Confusion Matrix': confusion_mat,\n",
    "    'Classification Report': classification_rep,\n",
    "    'ROC AUC': roc_auc,\n",
    "    'F1 Score': f1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.9863013698630136,\n",
       " 'Sensitivity (Recall)': 0.9824561403508771,\n",
       " 'Specificity': 1.0,\n",
       " 'Confusion Matrix': array([[16,  0],\n",
       "        [ 1, 56]]),\n",
       " 'Classification Report': '              precision    recall  f1-score   support\\n\\n           0       0.94      1.00      0.97        16\\n           1       1.00      0.98      0.99        57\\n\\n    accuracy                           0.99        73\\n   macro avg       0.97      0.99      0.98        73\\nweighted avg       0.99      0.99      0.99        73\\n',\n",
       " 'ROC AUC': 0.9912280701754386,\n",
       " 'F1 Score': 0.9911504424778761}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.98630137 0.95833333 0.97222222 0.93055556 0.97222222]\n",
      "Mean accuracy: 0.9639269406392694\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation and compute scores\n",
    "scores = cross_val_score(decision_tree, X, y, cv=kfold)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean accuracy:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train) #computes the mean and std\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the Logistic Regression classifier\n",
    "logistic_regression = LogisticRegression()\n",
    "\n",
    "# Train the classifier\n",
    "logistic_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs = logistic_regression.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_probs)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Calculate sensitivity (recall) and specificity\n",
    "tn, fp, fn, tp = confusion_mat.ravel()\n",
    "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "metrics_dict = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Sensitivity (Recall)': sensitivity,\n",
    "    'Specificity': specificity,\n",
    "    'Confusion Matrix': confusion_mat,\n",
    "    'Classification Report': classification_rep,\n",
    "    'ROC AUC': roc_auc,\n",
    "    'F1 Score': f1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.958904109589041,\n",
       " 'Sensitivity (Recall)': 1.0,\n",
       " 'Specificity': 0.8125,\n",
       " 'Confusion Matrix': array([[13,  3],\n",
       "        [ 0, 57]]),\n",
       " 'Classification Report': '              precision    recall  f1-score   support\\n\\n           0       1.00      0.81      0.90        16\\n           1       0.95      1.00      0.97        57\\n\\n    accuracy                           0.96        73\\n   macro avg       0.97      0.91      0.94        73\\nweighted avg       0.96      0.96      0.96        73\\n',\n",
       " 'ROC AUC': 0.90625,\n",
       " 'F1 Score': 0.9743589743589743}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression()\n",
    "steps = list()\n",
    "steps.append(('scaler',MinMaxScaler()))\n",
    "steps.append(('model',logistic_regression))\n",
    "pipeline = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.95890411 0.98611111 0.95833333 0.90277778 0.94444444]\n",
      "Mean accuracy: 0.9501141552511415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/adithya/anaconda3/envs/Major_Proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation and compute scores\n",
    "scores = cross_val_score(pipeline, X, y, cv=kfold)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean accuracy:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Major_Proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
